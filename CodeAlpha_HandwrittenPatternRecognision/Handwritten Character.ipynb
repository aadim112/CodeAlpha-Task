{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06355d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2edd54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:04<00:00, 2.39MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 136kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.24MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.54MB/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform= ToTensor())\n",
    "test_data = datasets.MNIST(root='data', train=False, download=True, transform= ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c4833f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f5fbb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "print(train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9fd8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHI9JREFUeJzt3Q90VeW55/HnJOQfSIIB868E5K8UMdgiYgYEFCYRZzGA1JFK7wWHgQGBEVIqK10Cop1JC3dRi40wvVeJ3ioovQIjy6aDQMJFE7gEGMpSKaFBwkCgskwCgYRA9qx3M0k5EKDvMclzcvb3s9ZeJ2ef/WRvNjvnd9693/0en+M4jgAA0MbC2nqFAAAYBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEtJG6ujpZvHixpKSkSExMjAwbNky2bdumvVmAGgIIaCPTp0+XVatWydSpU+VXv/qVhIeHy5NPPim7d+/W3jRAhY/BSIHWt3fvXrfFs3LlSlm0aJE7r7a2VgYNGiQJCQny2WefaW8i0OZoAQFt4He/+53b4pk1a1bTvOjoaJkxY4YUFRVJeXm56vYBGgggoA0cOHBA+vfvL7GxsX7zH374Yffx4MGDSlsG6CGAgDZw+vRpSU5Ovml+47xTp04pbBWgiwAC2sClS5ckKirqpvnmNFzj64DXEEBAGzDdrk037BuZjgiNrwNeQwABbcCcajOn4W7UOM/cGwR4DQEEtIEHH3xQ/vSnP0l1dbXf/D179jS9DngNAQS0gR/84Ady9epV+c1vftM0z5ySW7dunXt/UGpqqur2ARo6qKwV8BgTMk8//bRkZ2fL2bNnpW/fvvL222/L8ePH5c0339TePEAFIyEAbcR0OFiyZIn89re/lW+++UbS0tLk1VdflczMTO1NA1QQQAAAFVwDAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqgu5G1IaGBndo+s6dO4vP59PeHACAJXN3z/nz590xDsPCwtpPAJnwYVgSAGj/zDf9du/evf0EkGn5GCPkSekgEdqbAwCwdEXqZbd83PR+3uYBlJubKytXrpSKigoZPHiwvP76601fP3w7jafdTPh08BFAANDu/P/xde50GaVVOiG8//77kpWVJcuWLZP9+/e7AWTGuzKDMAIA0GoBtGrVKpk5c6Y899xzMnDgQFm7dq107NhR3nrrLfY6AKB1Aujy5ctSUlIiY8eObZpnekGY50VFRTctb74TxXxJ1/UTACD0tXgAff311+4XbyUmJvrNN8/N9aAb5eTkSFxcXNNEDzgA8Ab1G1HNF3RVVVU1TabbHgAg9LV4L7hu3bpJeHi4nDlzxm++eZ6UlHTT8lFRUe4EAPCWFm8BRUZGypAhQ2T79u1+oxuY5+np6S29OgBAO9Uq9wGZLtjTpk2Thx56yL3357XXXpOamhq3VxwAAK0WQM8884z85S9/kaVLl7odDx588EHJz8+/qWMCAMC7fI4ZNS6ImG7YpjfcaJnASAgA0A5dceqlQLa4HctiY2ODtxccAMCbCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgooPOagHYuvL4EOua08/XBbSu/5P+tnXN4KJp1jUpuZHWNeE791vXIDjRAgIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCwUgBBQ2jvmdds/qtX1vX9I0I7E+8IYCaA+nrrGuOPHTVuuYn9z5iXYPgRAsIAKCCAAIAhEYAvfzyy+Lz+fymAQMGtPRqAADtXKtcA7r//vvlk08++etKOnCpCQDgr1WSwQROUlJSa/xqAECIaJVrQEePHpWUlBTp3bu3TJ06VU6cOHHLZevq6qS6utpvAgCEvhYPoGHDhkleXp7k5+fLmjVrpKysTB599FE5f/58s8vn5ORIXFxc05SamtrSmwQA8EIAjRs3Tp5++mlJS0uTzMxM+fjjj6WyslI++OCDZpfPzs6Wqqqqpqm8vLylNwkAEIRavXdAly5dpH///lJaWtrs61FRUe4EAPCWVr8P6MKFC3Ls2DFJTk5u7VUBALwcQIsWLZLCwkI5fvy4fPbZZzJp0iQJDw+XH/7why29KgBAO9bip+BOnjzphs25c+fknnvukREjRkhxcbH7MwAArRZAGzZsaOlfCQS1+oyHrGtefOOfrWv6R0Ra1zQENKyoyJ/r661rqhrsr+V+L4DLv3XjhlrXxOz8o/2KzP6rrQ2oDn8bxoIDAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCAAQml9IB2gIj40NqK5m5ADrmoW/fM+65rGYC0H9eTHvm39nXbP9jXTrmk9fXm1ds+2f1lrXDPztPAlE78VFAdXhb0MLCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACggtGwEZJOvvOdgOr+bWhui29Le/RKwr9Z1+TfZT+C9nPHM6xr3r73E+ua2IHnrGvQ+mgBAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUMFgpAh6Vx4fYl2z/sFfB7SuMImUtvDcV2Osa/Z98l3rmj/OCGw/7LwUbV2TsO+SdU3pNwOsayL+x07rmjCfdQnaAC0gAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKhiMFG2qYdT3rGtWv2U/oGbfiMAO7QZpsK75j19Osq4J/0GNdU2X/+BY1wz853kSiP655dY1YeUHrGvu/lfrEqn/71eta/4l7S37FYnIf37sv1nXhO/cH9C6vIgWEABABQEEAGgfAbRr1y4ZP368pKSkiM/nk82bN/u97jiOLF26VJKTkyUmJkbGjh0rR48ebcltBgB4MYBqampk8ODBkpub2+zrK1askNWrV8vatWtlz5490qlTJ8nMzJTa2tqW2F4AQIiwvlI7btw4d2qOaf289tpr8tJLL8mECRPcee+8844kJia6LaUpU6Z8+y0GAISEFr0GVFZWJhUVFe5pt0ZxcXEybNgwKSoqaramrq5Oqqur/SYAQOhr0QAy4WOYFs/1zPPG126Uk5PjhlTjlJqa2pKbBAAIUuq94LKzs6WqqqppKi+3v/8AAODxAEpKSnIfz5w54zffPG987UZRUVESGxvrNwEAQl+LBlCvXr3coNm+fXvTPHNNx/SGS09Pb8lVAQC81gvuwoULUlpa6tfx4ODBgxIfHy89evSQBQsWyM9+9jPp16+fG0hLlixx7xmaOHFiS287AMBLAbRv3z557LHHmp5nZWW5j9OmTZO8vDx58cUX3XuFZs2aJZWVlTJixAjJz8+X6Ojolt1yAEC75nPMzTtBxJyyM73hRssE6eCL0N4c3IZvyP3WNWeW2g8kufehd61rSuokIDsuDLSu+fD1x61ruv5j87cl4M62/t+SNhlk1nhk399Z1yRM+FK87opTLwWyxe1Ydrvr+uq94AAA3kQAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAaB9fx4DQE9axY0B1V1ZUW9cUD/jQuqbsymXrmqyf/lgCcfe/nrCuSeh01rrGfkxwaHg4+SvrmuOtsiWhiRYQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQxGCrk06v6A6v4w4A1pC//lhYXWNZ03Fwe0risBVQEIBC0gAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKhiMFJL26sGA6sIC+Pzy3FdjrGtiNu+1rkHoivCFW9fUO4GtK9wXYCH+JrSAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqGAw0hBT+Xfp1jUvJf5DQOtqkEjrmpL/PdC6pod8Zl2D0FXvXLWuaZCGgNaV/4X98dpP9ge0Li+iBQQAUEEAAQDaRwDt2rVLxo8fLykpKeLz+WTz5s1+r0+fPt2df/30xBNPtOQ2AwC8GEA1NTUyePBgyc3NveUyJnBOnz7dNK1fv/7bbicAwOudEMaNG+dOtxMVFSVJSUnfZrsAACGuVa4BFRQUSEJCgtx3330yZ84cOXfu3C2Xraurk+rqar8JABD6WjyAzOm3d955R7Zv3y6/+MUvpLCw0G0xXb3afNfJnJwciYuLa5pSU1NbepMAAF64D2jKlClNPz/wwAOSlpYmffr0cVtFY8aMuWn57OxsycrKanpuWkCEEACEvlbvht27d2/p1q2blJaW3vJ6UWxsrN8EAAh9rR5AJ0+edK8BJScnt/aqAAChfAruwoULfq2ZsrIyOXjwoMTHx7vT8uXLZfLkyW4vuGPHjsmLL74offv2lczMzJbedgCAlwJo37598thjjzU9b7x+M23aNFmzZo0cOnRI3n77bamsrHRvVs3IyJBXX33VPdUGAEDAATR69GhxHOeWr//hD3+w/ZVoQVdi7GviwuwHFTWKau0/VPR+55R1zRXrCmgI69jRuubLfxgUwJpKrCum/vn29y7eyoAXyqxr7IdK9S7GggMAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAhMZXcsM7zl29y7rmyp+Pt8q2QH9k6yM/f8C65ssJv7au+f3FOOuaU7l9JRCdvykOqA5/G1pAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVDAYKQK26NOnrWv6S0mrbAua1zDqewHVnc26ZF3zxUP2A4uO+eMz1jWdnvizdU1nYVDRYEQLCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoGIw01PvuSsAA/h/xqxHrrmlzpH9C6IPLVK+nWNf/y96sCWlf/iEjrmu/vnWZdkzLpc+sahA5aQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQwGGmocexLGqQhoFWNijlnXbMgb4h1TZ919tsXUXFeAnFm1D3WNfHPnLSumd9ju3XNuI4l1jX/qyZRAvH3f3zCuqbb/+wU0LrgXbSAAAAqCCAAQPAHUE5OjgwdOlQ6d+4sCQkJMnHiRDly5IjfMrW1tTJ37lzp2rWr3HXXXTJ58mQ5c+ZMS283AMBLAVRYWOiGS3FxsWzbtk3q6+slIyNDampqmpZZuHChfPTRR7Jx40Z3+VOnTslTTz3VGtsOAPBKJ4T8/Hy/53l5eW5LqKSkREaOHClVVVXy5ptvynvvvSePP/64u8y6devku9/9rhtajzzySMtuPQDAm9eATOAY8fHx7qMJItMqGjt2bNMyAwYMkB49ekhRUVGzv6Ourk6qq6v9JgBA6As4gBoaGmTBggUyfPhwGTRokDuvoqJCIiMjpUuXLn7LJiYmuq/d6rpSXFxc05SamhroJgEAvBBA5lrQ4cOHZcOGDd9qA7Kzs92WVONUXl7+rX4fACCEb0SdN2+ebN26VXbt2iXdu3dvmp+UlCSXL1+WyspKv1aQ6QVnXmtOVFSUOwEAvMWqBeQ4jhs+mzZtkh07dkivXr38Xh8yZIhERETI9u1/vcvbdNM+ceKEpKent9xWAwC81QIyp91MD7ctW7a49wI1Xtcx125iYmLcxxkzZkhWVpbbMSE2Nlbmz5/vhg894AAAAQfQmjVr3MfRo0f7zTddradPn+7+/Mtf/lLCwsLcG1BND7fMzEx54403bFYDAPCADran4O4kOjpacnNz3QmhLdpnfwnxi3+/1rpm96PR1jVH65q/5ngnz8Udl2D1wqlHrWvyP3swoHX1e6E4oDrABmPBAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQDazzeiInglFpy1rln8XwP7ssBfJBVJWxgZfdm6ZkR0241qfaDO/nPcDwtnWdf0f67EuqafMKo1ghctIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoYjDTEXP3TMeuao0/fG9C6Bs6fb13z+X96XYLZgI+ft665742L1jX9D9gPLAqEGlpAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVPgcx3EkiFRXV0tcXJyMlgnSwRehvTkAAEtXnHopkC1SVVUlsbGxt1yOFhAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAII/gHJycmTo0KHSuXNnSUhIkIkTJ8qRI0f8lhk9erT4fD6/afbs2S293QAALwVQYWGhzJ07V4qLi2Xbtm1SX18vGRkZUlNT47fczJkz5fTp003TihUrWnq7AQDtXAebhfPz8/2e5+XluS2hkpISGTlyZNP8jh07SlJSUsttJQAg5Hyra0Dm61aN+Ph4v/nvvvuudOvWTQYNGiTZ2dly8eLFW/6Ouro692u4r58AAKHPqgV0vYaGBlmwYIEMHz7cDZpGzz77rPTs2VNSUlLk0KFDsnjxYvc60YcffnjL60rLly8PdDMAAO2Uz3EcJ5DCOXPmyO9//3vZvXu3dO/e/ZbL7dixQ8aMGSOlpaXSp0+fZltAZmpkWkCpqakyWiZIB19EIJsGAFB0xamXAtniniWLjY1t2RbQvHnzZOvWrbJr167bho8xbNgw9/FWARQVFeVOAABvsQog01iaP3++bNq0SQoKCqRXr153rDl48KD7mJycHPhWAgC8HUCmC/Z7770nW7Zsce8FqqiocOfHxcVJTEyMHDt2zH39ySeflK5du7rXgBYuXOj2kEtLS2utfwMAINSvAZmbSpuzbt06mT59upSXl8uPfvQjOXz4sHtvkLmWM2nSJHnppZduex7weuYakAk0rgEBQPvUKteA7pRVJnDMzaoAANwJY8EBAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFR0kCDjOI77eEXqRa79CABoR9z37+vez9tNAJ0/f9593C0fa28KAOBbvp/HxcXd8nWfc6eIamMNDQ1y6tQp6dy5s/h8Pr/XqqurJTU1VcrLyyU2Nla8iv1wDfvhGvbDNeyH4NkPJlZM+KSkpEhYWFj7aQGZje3evfttlzE71csHWCP2wzXsh2vYD9ewH4JjP9yu5dOITggAABUEEABARbsKoKioKFm2bJn76GXsh2vYD9ewH65hP7S//RB0nRAAAN7QrlpAAIDQQQABAFQQQAAAFQQQAEAFAQQAUNFuAig3N1fuvfdeiY6OlmHDhsnevXu1N6nNvfzyy+7wRNdPAwYMkFC3a9cuGT9+vDush/k3b9682e9105Fz6dKlkpycLDExMTJ27Fg5evSoeG0/TJ8+/abj44knnpBQkpOTI0OHDnWH6kpISJCJEyfKkSNH/Japra2VuXPnSteuXeWuu+6SyZMny5kzZ8Rr+2H06NE3HQ+zZ8+WYNIuAuj999+XrKwst2/7/v37ZfDgwZKZmSlnz54Vr7n//vvl9OnTTdPu3bsl1NXU1Lj/5+ZDSHNWrFghq1evlrVr18qePXukU6dO7vFh3oi8tB8MEzjXHx/r16+XUFJYWOiGS3FxsWzbtk3q6+slIyPD3TeNFi5cKB999JFs3LjRXd6MLfnUU0+J1/aDMXPmTL/jwfytBBWnHXj44YeduXPnNj2/evWqk5KS4uTk5DhesmzZMmfw4MGOl5lDdtOmTU3PGxoanKSkJGflypVN8yorK52oqChn/fr1jlf2gzFt2jRnwoQJjpecPXvW3ReFhYVN//cRERHOxo0bm5b54osv3GWKioocr+wHY9SoUc4LL7zgBLOgbwFdvnxZSkpK3NMq1w9Yap4XFRWJ15hTS+YUTO/evWXq1Kly4sQJ8bKysjKpqKjwOz7MIIjmNK0Xj4+CggL3lMx9990nc+bMkXPnzkkoq6qqch/j4+PdR/NeYVoD1x8P5jR1jx49Qvp4qLphPzR69913pVu3bjJo0CDJzs6WixcvSjAJutGwb/T111/L1atXJTEx0W++ef7ll1+Kl5g31by8PPfNxTSnly9fLo8++qgcPnzYPRfsRSZ8jOaOj8bXvMKcfjOnmnr16iXHjh2Tn/70pzJu3Dj3jTc8PFxCjfnqlgULFsjw4cPdN1jD/J9HRkZKly5dPHM8NDSzH4xnn31Wevbs6X5gPXTokCxevNi9TvThhx9KsAj6AMJfmTeTRmlpaW4gmQPsgw8+kBkzZqhuG/RNmTKl6ecHHnjAPUb69OnjtorGjBkjocZcAzEfvrxwHTSQ/TBr1iy/48F00jHHgflwYo6LYBD0p+BM89F8eruxF4t5npSUJF5mPuX1799fSktLxasajwGOj5uZ07Tm7ycUj4958+bJ1q1bZefOnX7fH2b+z81p+8rKSk8cD/NusR+aYz6wGsF0PAR9AJnm9JAhQ2T79u1+TU7zPD09XbzswoUL7qcZ88nGq8zpJvPGcv3xYb4R0vSG8/rxcfLkSfcaUCgdH6b/hXnT3bRpk+zYscP9/7+eea+IiIjwOx7MaSdzrTSUjgfnDvuhOQcPHnQfg+p4cNqBDRs2uL2a8vLynM8//9yZNWuW06VLF6eiosLxkh//+MdOQUGBU1ZW5nz66afO2LFjnW7durk9YELZ+fPnnQMHDriTOWRXrVrl/vzVV1+5r//85z93j4ctW7Y4hw4dcnuC9erVy7l06ZLjlf1gXlu0aJHb08scH5988onz/e9/3+nXr59TW1vrhIo5c+Y4cXFx7t/B6dOnm6aLFy82LTN79mynR48ezo4dO5x9+/Y56enp7hRK5txhP5SWljqvvPKK++83x4P52+jdu7czcuRIJ5i0iwAyXn/9dfegioyMdLtlFxcXO17zzDPPOMnJye4++M53vuM+NwdaqNu5c6f7hnvjZLodN3bFXrJkiZOYmOh+UBkzZoxz5MgRx0v7wbzxZGRkOPfcc4/bDblnz57OzJkzQ+5DWnP/fjOtW7euaRnzweP555937r77bqdjx47OpEmT3DdnL+2HEydOuGETHx/v/k307dvX+clPfuJUVVU5wYTvAwIAqAj6a0AAgNBEAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAANHw/wAh/GLbsaPqgwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[1]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "plt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\n",
    "plt.title(label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ef3683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_dataloader = DataLoader(test_data,batch_size=BATCH_SIZE,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e5e5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# Create a model with non-linear and linear layers\n",
    "class FashionMNISTModelV1(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # flatten inputs into single vector\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5da5c93",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FashionMNISTV1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model0 = \u001b[43mFashionMNISTV1\u001b[49m(input_shape=\u001b[32m784\u001b[39m,hidden_units=\u001b[32m10\u001b[39m,output_shape=\u001b[32m10\u001b[39m)\n\u001b[32m      2\u001b[39m model0.to(\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'FashionMNISTV1' is not defined"
     ]
    }
   ],
   "source": [
    "model0 = FashionMNISTV1(input_shape=784,hidden_units=10,output_shape=10)\n",
    "model0.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bb29746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading helper_functions.py\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11f97677",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhelper_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_fn\n\u001b[32m      3\u001b[39m loss_fn = nn.CrossEntropyLoss()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m optimizer = torch.optim.SGD(params=\u001b[43mmodel0\u001b[49m.parameters(),lr=\u001b[32m0.1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model0' is not defined"
     ]
    }
   ],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model0.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "  total_time = end - start\n",
    "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "  return total_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39147415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aamp8\\.conda\\envs\\machinelearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'timer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Set the seed and start the timer\u001b[39;00m\n\u001b[32m      5\u001b[39m torch.manual_seed(\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m train_time_start_on_cpu = \u001b[43mtimer\u001b[49m()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Set the number of epochs (we'll keep this small for faster training times)\u001b[39;00m\n\u001b[32m      9\u001b[39m epochs = \u001b[32m3\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'timer' is not defined"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Set the number of epochs (we'll keep this small for faster training times)\n",
    "epochs = 3\n",
    "\n",
    "# Create training and testing loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model0.train()\n",
    "        # 1. Forward pass\n",
    "        y_pred = model0(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulatively add up the loss per epoch\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model0(X)\n",
    "\n",
    "            # 2. Calculate loss (accumulatively)\n",
    "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "\n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "# Calculate training time\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model0.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa067b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_custom_image(image_path):\n",
    "    \"\"\"\n",
    "    Converts any image into MNIST-compatible tensor:\n",
    "    - Handles RGB / Grayscale\n",
    "    - Resizes to 28x28\n",
    "    - Auto-inverts background if needed\n",
    "    - Normalizes properly\n",
    "    \"\"\"\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    image = ImageOps.grayscale(image)\n",
    "\n",
    "    # Resize to MNIST dimensions\n",
    "    image = image.resize((28, 28))\n",
    "\n",
    "    # Convert to tensor\n",
    "    image_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "    # Auto-invert if background is white\n",
    "    # MNIST digits are white on black background\n",
    "    if image_tensor.mean() > 0.5:\n",
    "        image_tensor = 1.0 - image_tensor\n",
    "\n",
    "    # Normalize (MNIST stats)\n",
    "    image_tensor = transforms.Normalize(\n",
    "        mean=(0.1307,),\n",
    "        std=(0.3081,)\n",
    "    )(image_tensor)\n",
    "\n",
    "    # Add batch dimension\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0175df20",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/192869.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m image_tensor\n\u001b[32m     44\u001b[39m image_path = \u001b[33m\"\u001b[39m\u001b[33m/content/192869.png\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# change this\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m image_tensor = \u001b[43mpreprocess_custom_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessed image shape:\u001b[39m\u001b[33m\"\u001b[39m, image_tensor.shape)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mpreprocess_custom_image\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03mConverts any image into MNIST-compatible tensor:\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m- Handles RGB / Grayscale\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33;03m- Normalizes properly\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Convert to grayscale\u001b[39;00m\n\u001b[32m     20\u001b[39m image = ImageOps.grayscale(image)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aamp8\\.conda\\envs\\machinelearning\\Lib\\site-packages\\PIL\\Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/192869.png'"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageOps\n",
    "from torchvision import transforms\n",
    "\n",
    "# Redefine preprocess_custom_image within this cell to include the necessary imports\n",
    "# (This is done because the original function definition in KpiQfnpEpAdR was missing these imports,\n",
    "# and we are constrained to modify only this cell XGcc3_SK5yhZ.)\n",
    "def preprocess_custom_image(image_path):\n",
    "    \"\"\"\n",
    "    Converts any image into MNIST-compatible tensor:\n",
    "    - Handles RGB / Grayscale\n",
    "    - Resizes to 28x28\n",
    "    - Auto-inverts background if needed\n",
    "    - Normalizes properly\n",
    "    \"\"\"\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    image = ImageOps.grayscale(image)\n",
    "\n",
    "    # Resize to MNIST dimensions\n",
    "    image = image.resize((28, 28))\n",
    "\n",
    "    # Convert to tensor\n",
    "    image_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "    # Auto-invert if background is white\n",
    "    # MNIST digits are white on black background\n",
    "    if image_tensor.mean() > 0.5:\n",
    "        image_tensor = 1.0 - image_tensor\n",
    "\n",
    "    # Normalize (MNIST stats)\n",
    "    image_tensor = transforms.Normalize(\n",
    "        mean=(0.1307,),\n",
    "        std=(0.3081,)\n",
    "    )(image_tensor)\n",
    "\n",
    "    # Add batch dimension\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    return image_tensor\n",
    "\n",
    "image_path = \"/content/192869.png\"  # change this\n",
    "\n",
    "image_tensor = preprocess_custom_image(image_path)\n",
    "\n",
    "print(\"Processed image shape:\", image_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14684207",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_tensor.squeeze(), cmap=\"gray\")\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    logits = model0(image_tensor)\n",
    "    prediction = logits.argmax(dim=1).item()\n",
    "\n",
    "print(f\"Predicted digit: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fe19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76cc79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb99810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
